# ElasticSearch 실무 가이드

1. 노드 실행환경과 JVM 옵션
2. 힙 크기를 32GB 이하로 유지하는 이유
3. 엘라스틱서치와 가상 메모리
4. 분산환경에서의 메모리 스와핑
5. 시스템 튜닝 포인트

<br>

## 노드 실행환경과 JVM 옵션

<p>
엘라스틱서치는 빠르게 성장하고 있습니다. 2014년도에 1.0 버전이 공식적으로 출시한 후 현재 7버전 까지 다양한 기능들이 추가되고, 주기적으로 릴리즈되고 있습니다. 엘라스틱서치는 루씬을 기반으로 동작하기 때문에 엘라스틱서치의 버전에 따라 내부에서 사용하고 있는 루씬의 버전도 함께 달라집니다. 현재 자바 8이상의 버전에서 동작하고 있으며 그에 따라 64비트를 지원하여 힙 메모리도 과거에 비해 더 많이 할당하는 것이 가능해졌습니다. 다수의 CPU 사용은 물롱니고, 함수형 프로그래밍의 도입을 통해 업무 로직을 좀 더 편하고 쉽게 작성할 수 있게 되었습니다. 자바 8에서 제공하는 JVM 옵션은 다음과 같습니다.

<br>

<ol>
<li>-Xms : 기본 Heap 사이즈 (기본값: 64MB)</li>
<li>-Xmx : 최대 Heap 사이즈 (기본값: 256MB)</li>
<li>-XX:PermSize : 기본 Perm 사이즈</li>
<li>-XX:MaxPermSize : 최대 Perm 사이즈</li>
<li>-XX:NewSize : 최소 New 영역 사이즈</li>
<li>-XX:MaxNewSize : 최대 New 영역 사이즈</li>
<li>-XX:SurvivorRatio : New 영역 / Survivor 영역 비율 설정</li>
<li>-XX:NewRatio : Young Gen 과 Old Gen의 비율ㄹ 설정</li>
<li>-XX:DisableExplicitGC : System.gc() 무시 설정</li>
<li>-XX:CCMSPermGenSweepingEnabled : Perm Gen 영역도 GC의 대상이 되도록 설정</li>
<li>-XX:CMSClassUnloadingEnabled : Class 데이터도 GC의 대상이 되도록 설정</li>
</ol>

</p>
<br>

## 힙 크기를 32GB 이하로 유지하는 이유

<p>
엘라스틱서치는 메모리를 많이 사용하는 애플리케이션입니다. 시스템에서 제공되는 물리 메모리를 JVM 힙에 할당하여 엘라스틱서치가 사용하도록 설정할 수 있습니다. 일반적으로 힙 메모리가 많을수록 그에 비례하여 성능도 올라가는데, 너무 작은 힙 크기는 OOM 오류(Out of Memory)를 발생시키고 너무 큰 힙 크기는 SWT(Stop The World)를 발생시킵니다. 이번 절에서는 시스템에 탑재된 물리 메모리의 크기에 따라 엘라스틱서치에서 힙 크기를 얼마로 설정하면 좋을 지 알아보도록 하겠습니다.

<br>
</p>

엘라스틱서치를 기본 설정대로 설치하면 힙 크기가 1GB로 설정되어 있습니다. 이는 테스트를 위한 용도로 제공되는 값이며, 실제 운영환경에서는 반드시 1GB보다 큰 값으로 힙 크기를 변경해야 합니다. <code>config</code> 디렉터리를 살펴보면 <code>jvm.options</code> 파일이 존재합니다. 이 파일을 열어서 <code>Xms</code>와 <code>Xmx</code>를 수정하면 됩니다. JVM은 처음 실행될 때 Xms에 설정된 크기로 동작하다가 힙이 부족하다고 판단되면 Xmx에 설정된 힙 크기 까지 자동으로 늘어납니다. 이 과정에서 어플리케이션의 성능저하가 일어날 수 있기 때문에 처음부터 Xms와 Xmx의 크기를 같게 설정하는 것이 여러모로 유리합니다. <br>

자바 기반의 애플리케이션은 Object Pointer 정책이 적용됩니다. 이 정책은 객체의 메모리 번지를 표현하는 주솟값인데, 힙 크기가 32GB 이하인 경우 Compressed OOP 를 사용하게 됩니다. 보통은 64 시스템 기준의 경우 2^64 의 비트 주소의 공간까지 가르킬 수 있습니다. 주소 뿐 아니라 이동하는 값들도 64비트이기 때문에 32비트보다 연산속도 저하 및 메모리 공간의 낭비가 발생하였습니다. 하지만 Compressed OOP 는 논리적인 공간을 이용하여 포인터가 객체를 가르키는 것이 아니라 오프셋을 가르키기 때문에 2^32\*8 의 주소 공간을 32비트에서 사용할 수 있다는 장점이 있습니다. 따라서 기존보다 8배보다 더 큰 주소를 사용하면서 32비트의 값들을 표시하기 때문에 64비트 체계의 단점을 해결하는데 이바지하였습니다.

<br>

## 엘라스틱서치와 가상 메모리

### 가상 메모리

<p>
현대 운영체제에서는 애플리케이션이 물리 메모리를 직접적으로 할당받지 못합니다. 운영체제는 멀티태스킹 실현을 위해 애플리케이션을 위한 전용 가상 메모리를 만들고 할당하고 있습니다. 가상 메모리는 애플리케이션에서 물리적인 메모리 보다 많은 양의 메모리를 사용할 수 있도록 운영체제가 제공하는 메모리 관리 기술입니다. 특정 애플리케이션이 대용량의 가상 메모리를 할당받아 사용한다면 운영체제 성능에 큰 악영향을 끼칠수 있기 때문에 애플리케이션 별로 제한된 리소스의 한계가 있습니다. ulimit 명령어를 이용하면 리소스의 한계를 볼 수 있습니다. 가상 메모리도 리소스의 하나로써 virtual memory 속성으로 
수치화되어 제공하고 있습니다.
</p>

```
ulimit -a
```

<br>

### 엘라스틱서치를 위한 vm.max_map_count 설정

<p>
엘라스틱서치는 검색을 위해 루씬을 내장하고 있습니다. 루씬은 내부적으로 자바에서 제공하는 NIO 기술을 활용하는데, 이를 통해 커널에서 제공하는 mmap 시스템콜을 직접 호룿할 수 있으며 VM을 거치지 않고 직접 커널 모드로 진입할 수 있기에 높은 성능을 낼 수 있습니다. 따라서 엘라스틱서치에서 루씬을 원활하게 동작하기 위해서는 가상 메모리 설정 중 mmap항목을 변경해야 합니다. 리눅스에서는 다음 명령어로 기본 설정된 mmap 카운트를 조회할 수 있습니다
</p>

```
cat /proc/sys/vm/max_map_count
```

<br>

엘라스틱서치 내부에서는 vm.max_map_count 설정이 262114 미만이면 오류 메시지를 출력합니다. 따라서 해당 크기를 262114 이상으로 설정해야 합니다. <code>/etc/sysctl.conf</code>해당 파일로 이동한 후 아래를 추가 혹은 수정해주세요.

```
vm.max_map_count=262144
```

<br>

### 분산환경에서의 메모리 스와핑

<p>
대부분의 운영체제는 효율적인 메모리 관리를 위해 스와핑 기술을 사용합니다. 이를 이용해 사용하지 않는 애플리케이션의 물리 모메리를 디스크로 스왑하게 되는데, 스와핑이 일어나면 가상 메모리의 일부 내용을 디스크로 쓰기 위해 디스크의 일정 영역을 스왑 영역으로 만들게 됩니다. 이때 일어나는 동기화 작업에 의해  순간적으로 시스템 성능이 떨어지고 자칫 시스템 장애로 떨어질 수 있습니다.

스왑 상태를 확인하는 명령어는 다음과 같습니다.

</p>

```
free
```

<br>

생성된 스왑 영역이 디스크 상의 어디에 생성되어있는지 확인하는 명령어입니다.

```
cat /proc/swaps
```

<br>

스와핑이 발생할 경우 노드 안정성에 치명적이기 떄문에 이를 최대한 피해야 합니다. 메모리를 많이 사용하는 엘라스틱서치 특성상 스와핑 작업에 의해 가비지 컬렉션이 바정상적으로 수 분 동안 지속된다거나 노드의 응답이 느려질 수 있습니다. 또한 클러스터의 연결이 불안정해서 연결됨과 끊어짐이 반복되는 문제가 발생할 수 있습니다. 스와핑이라는 작업이 운영체제 차원에서 이루어지는 작업이기 때문에 엘라스틱 내부에 스와핑을 최소화하는 작업으로 boostrap.memory_lock 속성을 활성화하는 것을 추천합니다. 다음은 memory_lock 기능을 엘라스틱서치 내부에서 설정하는 방법입니다.

```
vi elasticsearch.yml

bootstrap.memory_lock: true
```

<br>

설정한 후 엘라스틱서치를 다시 실행시켜줍시다.

```
 systemctl restart elasticsearch.service
```

<br>

설정이 완료되었으면 memory_lock 이 정상적으로 수행되었는지 API를 통해 확인합니다.

```
GET _nodes?filter_path=**.mlockall
```

<br>

false로 설정되어 있으면 로그를 분석해봅니다. 보통 RLIMIT_MEMLOCK 값이 65,536 으로 설정되어 있으므로 이를 늘려야 합니다. 이를 해결하기 위해서는 ulimit 명령어를 통해 값을 unlimited로 변경하면 됩니다. 다음 명령어를 통해 max locked memory 값을 설정합니다.

```
ulimit -l unlimited
```

<br>

저는 아래와 같은 방법을 통해 max locked memory 의 제한을 해제하였습니다.

```
# we purpose revise with max locked memory
vi /usr/lib/systemd/system/elasticsearch.service

# add line Specifies the maximum memory size
LimitMEMLOCK=infinity

# restart deamon and services
systemctl daemon-reload
systemctl restart elasticsearch.service
```
